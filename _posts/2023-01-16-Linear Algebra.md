# Linear Algebra

- Linear Algebra and Its Applications, 6th Edition 의 내용을 정리하였다

*****

## Chapter 1 : Linear Equations in Linear Algebra

### 1) System of Linear Equations

linear equation : $a_1x_1 + a_2x_2 + \cdots + a_nx_n = b$ where $b$ and the coefficients $a_1,...,a_n$ are real or complex numbers <br>
A system of linear equations (linear system) : collection of linear equations involving the same variables
- solution of the linear system : a list $(s_1,...,s_n)$ that makes each equation a true statement when $s_1,...,s_n$ is substituted for $x_1,...,x_n$ 
- solution set of the linear system : the set of all possible solutions
  - equivalent : Two linear systems have the same solution set
  - consistent : A system of linear equations has unique solution or infinitely many solutions
  - inconsistent : A system of linear equations has no solution
  
The essential information of a linear system can be recorded compactly in a rectangular array called a matrix
- A system of linear equations <br>
  $1x_1 - 2x_2 + 1x_3 = \ \ 0$ <br>
  $0x_1 + 2x_2 - 8x_3 = \ \ 8$ <br>
  $5x_1 + 0x_2 - 5x_3 = 10$ 
- coefficient matrix ( matrix of coefficients ) <br>
  $ \begin{bmatrix} 1 & -2 & 1 \\\ 0 & 2 & -8 \\\ 5 & 0 & -5 \end{bmatrix} $
- augment matrix <br>
  $ \begin{bmatrix} 1 & -2 & 1 & 0 \\\ 0 & 2 & -8 & 8 \\\ 5 & 0 & -5 & 10 \end{bmatrix} $
  
elementary row operations
- (Replacement) replace one row by the sum of itself and a multiple of another row
- (Interchange) Interchange two rows
- (Scaling) Multiply all entries in a row by a nonzero constant

row equivalent : there is a sequence of elementary row operations that transforms one matrix into the other <br>
a system that is changed to a new one via row operation can be returned to original system via row operation. therefore, <br>
If the augmented matrices of two linear systems are row equivalent, then the two systems have the same solution set

### 2) Row Reduction and Echelon Forms

reduced (row) echelon form [ 1 ~ 3 : (row) echelon form ]
1. All nonzero rows are above any rwos of all zeros
2. Each leading entry of a row is in a column to the right of the leading entry of the row above it
3. All entries in a column below a leading entry are zeros
4. The leading entry in each nonzero row is $1$.
5. Each leading $1$ is the only nonzero entry in its column

Theorem 1 (Uniqueness of the Reduced Echelon Form) <br>
: Each matrix is row equivalent to one and only one reduced echelon matrix

pivot position in $A$ : a location in $A$ that corresponds to a leading 1 in the reduced echelon form of A <br>
A nonzero entry, or pivot, must be placed in pivot position <br>

pivot column of $A$ : a column of $A$ that contains a pivot position <br>

Solutions of Linear Systems
- basic variable : The variables corresponding to pivot columns in the matrix
- free variable : The other variable  [ we are free to choose any value for this varaible ]

Theorem 2 (Existence and Uniquenesss Theorem)
- A linear system is consistent if and only if an echelon form of the augmented matrix has no row of the form <br>
  $ \begin{bmatrix} 0 & \cdots & 0 & b \end{bmatrix} $ with $b$ nonzero
- If linear system is consistent
  - If there are no free variables, then the solution set contains a unique solution
  - If there is at least one free variable, then the solution set contains infinitely many solutions

### 3) Vector Equations

A matrix with only one column is called a column vector or simply a vector <br>
For all $u, v, w$ in $R^n$ and all scalars $c$ and $d$:
- $u+v = v+u$
- $(u+v)+w = u+(v+w)$
- $u+0=0+u=u$
- $u+(-u)=-u+u=0$ where $-u$ denotes $(-1)u$
- $c(u+v) = cu+cv$
- $(c+d)u = cu+du$
- $c(du) = (cd)u$
- $1u = u$

linear combination of $v_1,...,v_p$ with weights $c_1,...,c_p$ : $y = c_1v_1 + \cdots + c_pv_p$ <br>
- A vector equation $x_1a_1 + x_2a_2 + \cdots + x_na_n = b$ has the same solution set <br>
  as the linear system whose augmented matrix is $ \begin{bmatrix} a_1 & a_2 & \cdots & a_n & b \end{bmatrix} $
- In particular, $b$ can be generated by a linear combination of $a_1,...,a_n$ if and only if <br>
  there exists a solution to the linear system corresponding to the above matrix

If $v_1,...,v_p$ are in $R^n$, then the set of all linear combinations of $v_1,...,v_p$ is decoted by $Span\{v_1,...,v_p\}$ <br>
and is called the subset of $R^n$ spanned (generated) by $v_1,...,v_p$ <br>
That is, $Span\{v_1,...,v_p\}$ is the collection of all vectors that can be written in the form <br>
$c_1v_1 + c_2v_2 + \cdots + c_pv_p$ with $c_1,...,c_p$ scalars

### 4) The Matrix Equation

If $A$ is an $m \times n$ matrix, with columns $a_1, ..., a_n$, and if $x$ is in $R^n$, <br>
then $Ax$ is the linear combination of the columns of $A$ using the corresponding entries in $x$ as weights 
- Theorem 3
  - If $b$ is in $R^m$, the matrix equation $Ax = b$ has the same solution set <br>
    as the vector equation $x_1a_1 + x_2a_2 + \cdots + x_na_n = b$
- Theorem 4 : for a particular $A$, the following statements are logically equivalent
  - For each $b$ in $R^m$, the equation $Ax = b$ has a solution
  - Each $b$ in $R^m$ is a linear combination of the columns of $A$
  - The columns of $A$ span $R^m$
  - $A$ has a pivot position in every row
  
Theorem 5 : If $A$ is an $m \times n$, $u$ and $v$ are vectors in $R^n$, and $c$ is a scalar, then: <br>
- $A(u+v) = Au+Av$
- $A(cu) = c(Au)$

### 5) Solution Sets of Linear Systems

A system of linear euqations is homogeneous : it can be written in the form $Ax = 0$
- trivial solution : $Ax = 0$ always has at least one solution, namely $x = 0$
- nontrivial solution : nonzero vector $x$ that satisfies $Ax = 0$
  - $Ax = 0$ has a nontrivial solution if and only if the equation has at least one free variable

Let's describe all solutions of $Ax = b$, where $ A = \begin{bmatrix} 3 & 5 & -4 \\\ -3 & -2 & 4 \\\ 6 & 1 & -8 \end{bmatrix}$ and $b = \begin{bmatrix} 7 \\\ -1 \\\ -4 \end{bmatrix} $
- $ \begin{bmatrix} A & b \end{bmatrix} = \begin{bmatrix} 3 & 5 & -4 & 7 \\\ -3 & -2 & 4 & -1 \\\ 6 & 1 & -8 & -4 \end{bmatrix} ~ \begin{bmatrix} 1 & 0 & -4/3 & -1 \\\ 0 & 1 & 0 & 2 \\\ 0 & 0 & 0 & 0 \end{bmatrix} $
- therefore, $ x = \begin{bmatrix} x_1 \\\ x_2 \\\ x_3 \end{bmatrix} = \begin{bmatrix} -1 + (4/3)x_3 \\\ 2 \\\ x_3 \end{bmatrix} = \begin{bmatrix} -1 \\\ 2 \\\ 0 \end{bmatrix} + \begin{bmatrix} (4/3)x_3 \\ 0 \\ x_3 \end{bmatrix} = \begin{bmatrix} -1 \\\ 2 \\\ 0 \end{bmatrix} + x_3 \begin{bmatrix} 4/3 \\\ 0 \\\ 1 \end{bmatrix} $
- therefore, the solution set of $Ax = b$ in parametric vector form is $x = p + tv$ <br>
  where $p = \begin{bmatrix} -1 \\\ 2 \\\ 0 \end{bmatrix}$, $v = \begin{bmatrix} 4/3 \\\ 0 \\\ 1 \end{bmatrix}$, and $t$ is in $R$, because $x_3$ is free variable
- and, the solution set of $Ax = 0$ in parametric vector form is $x = tv$ <br>
  because $p$ is the solution of $Ax = b$, and $v$ is the solution of $Ax = 0$
- Theorem 6
  - Suppose the equation $Ax = b$ is consistent for some given $b$, and let $p$ be a solution
  - Then the solution set of $Ax = b$ is the set of all vectors of the form $w = p + v_h$, <br>
    where v_h is any solution of the homogeneous equation $Ax = 0$

### 6) Applications of Linear Systems

(pdf 87p 13번 문제로, 나중에 img 따로 넣자) <br>

$A : 30 + x_2 = 80 + x_1 \Longrightarrow -x_1 + x_2 = 50$ <br>
$B : x_3 + x_5 = x_2 + x_4 \Longrightarrow -x_2 + x_3 - x_4 + x_5 = 0$ <br>
$C : 100 + x_6 = 40 + x_5 \Longrightarrow -x_5 + x_6 = -60$ <br>
$D : 40 + x_4 = 90 + x_6 \Longrightarrow x_4 - x_6 = 50$ <br>
$E : 60 + x_1 = 20 + x_3 \Longrightarrow x_1 - x_3 = -40$ <br>
<br>
row reduce the augmented matrix of the linear system: <br>
$\begin{bmatrix}
-1 & 1 & 0 & 0 & 0 & 0 & 50 \\\
0 & -1 & 1 & -1 & 1 & 0 & 0 \\\
0 & 0 & 0 & 0 & -1 & 1 & -60 \\\
0 & 0 & 0 & 1 & 0 & -1 & 50 \\\
1 & 0 & -1 & 0 & 0 & 0 & -40
\end{bmatrix}$
~
$\begin{bmatrix}
1 & 0 & -1 & 0 & 0 & 0 & -40 \\\
0 & 1 & -1 & 0 & 0 & 0 & 10 \\\
0 & 0 & 0 & 1 & 0 & -1 & 50 \\\
0 & 0 & 0 & 0 & 1 & -1 & 60 \\\
0 & 0 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}$
, 
$\therefore x = \begin{bmatrix} x_1 \\\ x_2 \\\ x_3 \\\ x_4 \\\ x_5 \\\ x_6 \end{bmatrix}
= \begin{bmatrix} x_3 - 40 \\\ x_3 + 10 \\\ x_3 \\\ x_6 + 50 \\\ x_6 + 60 \\\ x_6 \end{bmatrix}
= \begin{bmatrix} -40 \\\ 10 \\\ 0 \\\ 50 \\\ 60 \\\ 0 \end{bmatrix} + x_3 \begin{bmatrix} 1 \\\ 1 \\\ 1 \\\ 0 \\\ 0 \\\ 0 \end{bmatrix} + x_6 \begin{bmatrix} 0 \\\ 0 \\\ 0 \\\ 1 \\\ 1 \\\ 1 \end{bmatrix}$, where $x_3 \geq 40$

### 7) Linear Independence

An indexed set of vectors $\{v_1,..,v_p\}$ in $R^n$ is said to be
- linearly independent : $x_1v_1 + x_2v_2 + \cdots + x_pv_p = Ax = 0 \ $ has only the trivial solution <br>
- linearly dependent : there exist weights $c_1,...,c_p$, not all zero, such that $c_1v_1 + c_2v_2 + \cdots + c_pv_p = 0$
  - $c_1v_1 + c_2v_2 + \cdots + c_pv_p = 0$ is called a linear dependence relation
  
Theorem 7 : Characterization of Linearly Dependent Sets
- An indexed set $S = \{v_1,...,v_p\}$ of two or more vectors is linearly dependent if and only if <br>
  at least one of the vectors in $S$ is a linear combination of the others.
- if S is linearly dependent and $v_1 \neq 0$, <br>
  then some $v_j$ (with $j>1$) is a linear combination of the preceding vectors, $v_1,...,v_{j-1}$
  
Theorem 8 : If a set contains more vectors than there are entries in each vector, then the set is linearly dependent
- any set $\{v_1,...,v_p\}$ in $R^n$ is linearly dependent if $p > n$

Theorem 9 : If a set $S = \{v_1,...,v_p\}$ in $R^n$ contains the zero vector, then the set is linearly dependent

### 8) Introduction to Linear Transformations

Transformation (function, mapping) $\ T : V \rightarrow W$ : a rule that assigns to each vector $x$ in $V$ a vector $T(x)$ in $W$
- domain of $T$ : $V$
- codomain of $T$ : $W$
- image of $x$ : the vector $T(x)$
- range of $T$ : the set of all images $T(x)$

Linear Transformation : transformation $T$ that has the properties
- $T(u+v) = T(u) + T(v)$ for all $u, v$ in the domain of $T$
- $T(cu) = cT(u)$ for all scalars $c$ and all $u$ in the domain of $T$

Matrix Transformation : $T(x) = Ax$ where $A$ is an $m \times n$ matrix, denoted by $x \mapsto Ax$
- The range of $T$ is the set of all linear combinations of the columns of $A$
- Every matrix transformation is linear transformation

### 9) The Matrix of a Linear Transformation

Theorem 10
- Let $T : R^n \rightarrow R^m$ be a linear transformation. <br>
  Then there exists a unique matrix $A$ such that $T(x) = Ax$ for all $x$ in $R^n$
- $A$ is the $m \times n$ matrix whose $j$th column is the vector $T(e_j)$, where $e_j$ is the $j$th column of the identity matrix in $R^n$: <br>
  $A = \begin{bmatrix} T(e_1) & \cdots & T(e_n) \end{bmatrix}$, called by the standard matrix for $T$
- Warning : Every linear transformation is not matrix transformation
  - $T$ is matrix transformation if $T$ is linear transformation from $R^n$ to $R^m$

A mapping $T : R^n \rightarrow R^m$ is said to be
- onto : each $b$ in $R^m$ is the image of at **least** one $x$ in $R^n$
  - if $T(x) = Ax$, then 'onto' is that $Ax = b$ has a solution for each $b$
- one-to-one : each $b$ in $R^m$ is the image of at **most** one $x$ in $R^n$
  - if $T(x) = Ax$, then 'one-to-one' is that $Ax = b$ has not a free variable

Theorem 11 : Let $T : R^n \rightarrow R^m$ be a linear transformation. Then:
- $T$ is one-to-one if and only if the equation $T(x) = 0$ has only the trivial solution

Theorem 12 : Let $T : R^n \rightarrow R^m$ be a linear transformation, and let $A$ be the standard matrix for $T$. Then:
- $T$ maps $R^n$ onto $R^m$ if and only if the columns of $A$ span $R^m$
- $T$ is one-to-one if and only if the columns of $A$ are linearly independent

### 10) Linear Models in Business, Science, and Engineering

In a certain region, about 7% of a city's population moves to the surrounding suburbs each year, <br>
and about 5% of the suburban population moves into the city. <br>
In 2020, there were 800,000 residents in the city and 500,000 in the suburbs. <br>
Set up a difference equation that describes this situation, where $x_0$ is the initial population in 2020. <br>
Then estimate the populations in the city and in the suburbs two years later, in 2022
- $x_0 = \begin{bmatrix} city \\\ suburbs \end{bmatrix} = \begin{bmatrix} 800,000 \\\ 500,000 \end{bmatrix}$
- difference equation (recurrence relation) : $x_{k+1} = Mx_k$ for $k = 0, 1, 2, ...$ 
  - migration matrix $M = \begin{bmatrix} city \rightarrow city & suburbs \rightarrow city \\\ city \rightarrow suburbs & suburbs \rightarrow subrubs \end{bmatrix} = \begin{bmatrix} 0.93 & 0.05 \\\ 0.07 & 0.95 \end{bmatrix}$
- $\therefore \ x_2 = {\begin{bmatrix} 0.93 & 0.05 \\\ 0.07 & 0.95 \end{bmatrix}}^2 x_0 = \begin{bmatrix} 741,720 \\\ 558,280 \end{bmatrix}$

*****

## Chapter 2 : Matrix Algebra

### 1) Matrix Operations

### 2) The Inverse of a Matrix

### 3) Characterizations of Invertible Matrices

### 4) Partitioned Matrices

### 5) Matrix Factorizations

### 6) The Leontief Input-Output Model

### 7) Applications to Computer Graphics

### 8) Subspaces of $R^n$

### 9) Dimension and Rank

### Chapter 2 Project

*****

## Chapter 3 : Determinants

### 1) Introduction to Determinants

### 2) Properties of Determinants

### 3) Cramer's Rule, Volumn, and Linear Transformations

### Chapter 3 Project

*****

## Chapter 4 : Vector Spaces

### 1) Vector Spaces and Subspaces

### 2) Null Spaces, Column Spaces, Row Spaces, and Linear Transformations

### 3) Linearly Independent Sets; Bases

### 4) Coordinate Systems

### 5) The Dimension of a Vector Space

### 6) Change of Basis

### 7) Digital Signal Processing

### 8) Applications to Difference Equations

### Chapter 4 Project

*****

## Chapter 5 : Eigenvalues and Eigenvectors

### 1) Eigenvectors and Eigenvalues

### 2) The Characteristic Equation

### 3) Diagonalization

### 4) Eigenvectors and Linear Transformations

### 5) Complex Eigenvalues

### 6) Discrete Dynamical Systems

### 7) Applications to Differential Equations

### 8) Iterative Estimates for Eigenvalues

### 9) Applications to Markov Chains

### Chapter 5 Project

*****

## Chapter 6 : Orthogonality and Least Squares

### 1) Inner Product, Length, and Orthogonality

### 2) Orthogonal Sets

### 3) Orthogonal Projections

### 4) The Gram-Schmidt Process

### 5) Least-Squares Problems

### 6) Machine Learning and Linear Models

### 7) Inner Product Spaces

### 8) Applications of Inner Product Spaces

### Chapter 6 Project

*****

## Chapter 7 : Symmetric Matrices and Quadratic Forms

### 1) Diagonalization of Symmetric Matrices

### 2) Quadratic Forms

### 3) Constrained Optimization

### 4) The Singular Value Decomposition

### 5) Applications to Image Processing and Statistics

### Chapter 7 Project

*****

## Chapter 8 : The Geometry of Vector Spaces

### 1) Affine Combinations

### 2) Affine Independence

### 3) Convex Combinations

### 4) Hyperplanes

### 5) Polytopes

### 6) Curves and Surfaces

### Chapter 8 Project

*****

## Chapter 9 : Optimization

### 1) Matrix Games

### 2) Linear Programming ─ Geometric Method

### 3) Linear Programming ─ Simplex Method

### 4) Duality

### Chapter 9 Project

*****

## Chapter 10 : Finite-State Markov Chains

### 1) Introduction and Examples

### 2) The Steady-State Vector and Google's PageRank

### 3) Communication Classes

### 4) Classification of States and Periodicity

### 5) The Fundamental Matrix

### 6) Markov Chains and Baseball Statistics
